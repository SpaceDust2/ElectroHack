"""
üîç –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–∞—Ä—É—à–∏—Ç–µ–ª–µ–π
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')


def load_and_analyze_dataset(file_path: str):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π")

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    rows = []
    for item in data:
        row = {
            'accountId': item.get('accountId'),
            'buildingType': item.get('buildingType'),
            'roomsCount': item.get('roomsCount'),
            'residentsCount': item.get('residentsCount'),
            'totalArea': item.get('totalArea'),
            'isCommercial': item.get('isCommercial', False)
        }

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–æ –º–µ—Å—è—Ü–∞–º
        consumption = item.get('consumption', {})
        for month in range(1, 13):
            row[f'month_{month}'] = consumption.get(str(month), 0)

        rows.append(row)

    df = pd.DataFrame(rows)
    return df


def calculate_consumption_features(df):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è"""
    print("üîß –í—ã—á–∏—Å–ª—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è...")

    # –ú–µ—Å—è—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    month_cols = [f'month_{i}' for i in range(1, 13)]

    # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    df['avg_consumption'] = df[month_cols].mean(axis=1)
    df['min_consumption'] = df[month_cols].min(axis=1)
    df['max_consumption'] = df[month_cols].max(axis=1)
    df['std_consumption'] = df[month_cols].std(axis=1)
    df['cv'] = df['std_consumption'] / \
        df['avg_consumption']  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏

    # –°–µ–∑–æ–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    df['winter_avg'] = df[['month_12', 'month_1', 'month_2']].mean(
        axis=1)  # –î–µ–∫–∞–±—Ä—å-–§–µ–≤—Ä–∞–ª—å
    df['spring_avg'] = df[['month_3', 'month_4', 'month_5']].mean(
        axis=1)   # –ú–∞—Ä—Ç-–ú–∞–π
    df['summer_avg'] = df[['month_6', 'month_7', 'month_8']].mean(
        axis=1)   # –ò—é–Ω—å-–ê–≤–≥—É—Å—Ç
    df['autumn_avg'] = df[['month_9', 'month_10', 'month_11']].mean(
        axis=1)  # –°–µ–Ω—Ç—è–±—Ä—å-–ù–æ—è–±—Ä—å

    # –û—Ç–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–∑–æ–Ω (–æ–∫—Ç—è–±—Ä—å-–∞–ø—Ä–µ–ª—å) - –∫–ª—é—á–µ–≤–æ–π –∫—Ä–∏—Ç–µ—Ä–∏–π!
    df['heating_season'] = df[['month_10', 'month_11', 'month_12',
                               'month_1', 'month_2', 'month_3', 'month_4']].mean(axis=1)

    # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
    df['summer_winter_ratio'] = df['summer_avg'] / (df['winter_avg'] + 1e-8)
    df['heating_summer_ratio'] = df['heating_season'] / \
        (df['summer_avg'] + 1e-8)

    # –ù—É–ª–µ–≤—ã–µ –º–µ—Å—è—Ü—ã
    df['zero_months'] = (df[month_cols] == 0).sum(axis=1)
    df['months_with_data'] = (df[month_cols] > 0).sum(axis=1)

    # –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –Ω–∞ –∂–∏—Ç–µ–ª—è –∏ –ø–ª–æ—â–∞–¥—å
    df['consumption_per_resident'] = df['avg_consumption'] / \
        (df['residentsCount'] + 1e-8)
    df['consumption_per_area'] = df['avg_consumption'] / \
        (df['totalArea'].fillna(df['roomsCount'] * 17.5) + 1e-8)

    # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ù–ï–û–ß–ï–í–ò–î–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò

    # –¢—Ä–µ–Ω–¥—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    df['q1_avg'] = df[['month_1', 'month_2', 'month_3']].mean(axis=1)
    df['q2_avg'] = df[['month_4', 'month_5', 'month_6']].mean(axis=1)
    df['q3_avg'] = df[['month_7', 'month_8', 'month_9']].mean(axis=1)
    df['q4_avg'] = df[['month_10', 'month_11', 'month_12']].mean(axis=1)

    # –ú–µ–∂–∫–≤–∞—Ä—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
    quarter_cols = ['q1_avg', 'q2_avg', 'q3_avg', 'q4_avg']
    df['quarter_stability'] = df[quarter_cols].std(
        axis=1) / (df[quarter_cols].mean(axis=1) + 1e-8)

    # –ü–∏–∫–æ–≤—ã–µ –º–µ—Å—è—Ü—ã    avg_threshold_high = df['avg_consumption'].values[:, np.newaxis] * 1.5    avg_threshold_low = df['avg_consumption'].values[:, np.newaxis] * 0.5    df['peak_months'] = (df[month_cols].values > avg_threshold_high).sum(axis=1)    df['low_months'] = (df[month_cols].values < avg_threshold_low).sum(axis=1)

    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –≤—ã—Å–æ–∫–∏–µ –º–µ—Å—è—Ü—ã
    high_threshold = df['avg_consumption'] * 1.2
    df['consecutive_high'] = 0
    for idx in df.index:
        max_consecutive = 0
        current_consecutive = 0
        for month in range(1, 13):
            if df.loc[idx, f'month_{month}'] > high_threshold.loc[idx]:
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 0
        df.loc[idx, 'consecutive_high'] = max_consecutive

    # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
    df['max_min_ratio'] = df['max_consumption'] / \
        (df['min_consumption'] + 1e-8)
    df['median_mean_ratio'] = df[month_cols].median(
        axis=1) / (df['avg_consumption'] + 1e-8)

    # –≠–Ω—Ç—Ä–æ–ø–∏—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è (–º–µ—Ä–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏)
    def calculate_entropy(row):
        values = row[month_cols].values + 1e-8  # –ò–∑–±–µ–≥–∞–µ–º log(0)
        probabilities = values / values.sum()
        return -np.sum(probabilities * np.log2(probabilities))

    df['consumption_entropy'] = df.apply(calculate_entropy, axis=1)

    return df


def discover_hidden_patterns(df):
    """–ü–æ–∏—Å–∫ —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª"""
    print("\nüïµÔ∏è –ü–û–ò–°–ö –°–ö–†–´–¢–´–• –ü–ê–¢–¢–ï–†–ù–û–í")
    print("=" * 60)

    commercial = df[df['isCommercial'] == True]
    residential = df[df['isCommercial'] == False]

    hidden_patterns = {}

    # 1. –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó
    print("\nüîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑:")

    # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [col for col in numeric_cols if col not in ['accountId']]

    correlation_with_target = df[numeric_cols + ['isCommercial']
                                 ].corr()['isCommercial'].abs().sort_values(ascending=False)

    print("   –¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ–º:")
    for i, (feature, corr) in enumerate(correlation_with_target.head(11).items()):
        if feature != 'isCommercial':
            print(f"   {i}. {feature}: {corr:.3f}")

    hidden_patterns['top_correlations'] = correlation_with_target.head(
        11).to_dict()

    # 2. –ú–ê–®–ò–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –î–õ–Ø –ü–û–ò–°–ö–ê –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
    print("\nü§ñ ML-–∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X = df[numeric_cols].fillna(0)
    y = df['isCommercial'].astype(int)

    # Random Forest –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)

    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)

    print("   –¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (Random Forest):")
    for i, row in enumerate(feature_importance.head(10).iterrows()):
        feature, importance = row[1]['feature'], row[1]['importance']
        print(f"   {i+1}. {feature}: {importance:.3f}")

    hidden_patterns['rf_importance'] = feature_importance.head(
        15).to_dict('records')

    # 3. –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –†–ê–ó–õ–ò–ß–ò–ô
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏:")

    from scipy import stats

    significant_features = []
    for feature in numeric_cols:
        if feature not in ['isCommercial']:
            comm_values = commercial[feature].dropna()
            res_values = residential[feature].dropna()

            if len(comm_values) > 0 and len(res_values) > 0:
                # t-test
                t_stat, p_value = stats.ttest_ind(
                    comm_values, res_values, equal_var=False)

                if p_value < 0.01:  # –ó–Ω–∞—á–∏–º–æ—Å—Ç—å 1%
                    effect_size = (comm_values.mean() - res_values.mean()) / \
                        np.sqrt((comm_values.var() + res_values.var()) / 2)
                    significant_features.append({
                        'feature': feature,
                        'p_value': p_value,
                        'effect_size': effect_size,
                        'comm_mean': comm_values.mean(),
                        'res_mean': res_values.mean()
                    })

    significant_features = sorted(
        significant_features, key=lambda x: x['p_value'])

    print("   –ù–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Ä–∞–∑–ª–∏—á–∏—è:")
    for i, feat in enumerate(significant_features[:10]):
        print(
            f"   {i+1}. {feat['feature']}: p={feat['p_value']:.2e}, effect={feat['effect_size']:.2f}")

    hidden_patterns['significant_differences'] = significant_features[:15]

    # 4. –ö–û–ú–ë–ò–ù–ê–¶–ò–û–ù–ù–´–ï –ü–†–ê–í–ò–õ–ê
    print("\nüîÑ –ü–æ–∏—Å–∫ –∫–æ–º–±–∏–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª:")

    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø—Ä–∞–≤–∏–ª
    combo_rules = []

    # –ü—Ä–∞–≤–∏–ª–æ: –≤—ã—Å–æ–∫–æ–µ + —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ
    mask1 = (df['avg_consumption'] >
             df['avg_consumption'].quantile(0.8)) & (df['cv'] < 0.3)
    precision1, recall1 = calculate_rule_metrics(df, mask1)
    combo_rules.append({
        'rule': '–í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ + —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å',
        'condition': 'avg_consumption > 80% –∫–≤–∞–Ω—Ç–∏–ª—å AND cv < 0.3',
        'precision': precision1,
        'recall': recall1,
        'count': mask1.sum()
    })

    # –ü—Ä–∞–≤–∏–ª–æ: –æ—Ç–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–∑–æ–Ω + –Ω–µ—Ç –ø–∏–∫–æ–≤
    mask2 = (df['heating_season'] > 2000) & (df['peak_months'] < 2)
    precision2, recall2 = calculate_rule_metrics(df, mask2)
    combo_rules.append({
        'rule': '–í—ã—Å–æ–∫–∏–π –æ—Ç–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–∑–æ–Ω + –Ω–µ—Ç –ø–∏–∫–æ–≤',
        'condition': 'heating_season > 2000 AND peak_months < 2',
        'precision': precision2,
        'recall': recall2,
        'count': mask2.sum()
    })

    # –ü—Ä–∞–≤–∏–ª–æ: —ç–Ω—Ç—Ä–æ–ø–∏—è + –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    mask3 = (df['consumption_entropy'] < df['consumption_entropy'].quantile(
        0.3)) & (df['consecutive_high'] > 6)
    precision3, recall3 = calculate_rule_metrics(df, mask3)
    combo_rules.append({
        'rule': '–ù–∏–∑–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è + –¥–æ–ª–≥–∏–µ –≤—ã—Å–æ–∫–∏–µ –ø–µ—Ä–∏–æ–¥—ã',
        'condition': 'consumption_entropy < 30% –∫–≤–∞–Ω—Ç–∏–ª—å AND consecutive_high > 6',
        'precision': precision3,
        'recall': recall3,
        'count': mask3.sum()
    })

    combo_rules = sorted(
        combo_rules, key=lambda x: x['precision'] * x['recall'], reverse=True)

    print("   –õ—É—á—à–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞:")
    for i, rule in enumerate(combo_rules):
        print(f"   {i+1}. {rule['rule']}")
        print(
            f"      –¢–æ—á–Ω–æ—Å—Ç—å: {rule['precision']:.2%}, –ü–æ–∫—Ä—ã—Ç–∏–µ: {rule['recall']:.2%}")
        print(f"      –£—Å–ª–æ–≤–∏–µ: {rule['condition']}")

    hidden_patterns['combination_rules'] = combo_rules

    # 5. –ê–ù–û–ú–ê–õ–¨–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´
    print("\nüö® –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:")

    anomaly_patterns = []

    # –ê–Ω–æ–º–∞–ª–∏—è: –Ω—É–ª–µ–≤–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ + –≤—ã—Å–æ–∫–∏–µ –¥—Ä—É–≥–∏–µ –º–µ—Å—è—Ü—ã
    zero_but_high = df[(df['zero_months'] > 0) &
                       (df['max_consumption'] > 1000)]
    if len(zero_but_high) > 0:
        fraud_rate = zero_but_high['isCommercial'].mean()
        anomaly_patterns.append({
            'pattern': '–ù—É–ª–µ–≤—ã–µ –º–µ—Å—è—Ü—ã + –≤—ã—Å–æ–∫–∏–µ –ø–∏–∫–∏',
            'count': len(zero_but_high),
            'fraud_rate': fraud_rate,
            'description': '–û–±—ä–µ–∫—Ç—ã —Å –Ω—É–ª–µ–≤—ã–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ–º –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ—Å—è—Ü—ã, –Ω–æ –≤—ã—Å–æ–∫–∏–º –≤ –¥—Ä—É–≥–∏–µ'
        })

    # –ê–Ω–æ–º–∞–ª–∏—è: –æ—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
    low_entropy = df[df['consumption_entropy'] <
                     df['consumption_entropy'].quantile(0.1)]
    if len(low_entropy) > 0:
        fraud_rate = low_entropy['isCommercial'].mean()
        anomaly_patterns.append({
            'pattern': '–°–≤–µ—Ä—Ö–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ',
            'count': len(low_entropy),
            'fraud_rate': fraud_rate,
            'description': '–ö—Ä–∞–π–Ω–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è'
        })

    for pattern in anomaly_patterns:
        print(
            f"   üìç {pattern['pattern']}: {pattern['count']} –æ–±—ä–µ–∫—Ç–æ–≤, {pattern['fraud_rate']:.1%} –º–æ—à–µ–Ω–Ω–∏–∫–∏")

    hidden_patterns['anomaly_patterns'] = anomaly_patterns

    return hidden_patterns


def calculate_rule_metrics(df, mask):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–∫—Ä—ã—Ç–∏—è –ø—Ä–∞–≤–∏–ª–∞"""
    if mask.sum() == 0:
        return 0, 0

    tp = (mask & df['isCommercial']).sum()
    fp = (mask & ~df['isCommercial']).sum()
    fn = (~mask & df['isCommercial']).sum()

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0

    return precision, recall


def analyze_commercial_patterns(df):
    """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    print("\nüîç –ê–ù–ê–õ–ò–ó –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–• –ü–ê–¢–¢–ï–†–ù–û–í")
    print("=" * 60)

    commercial = df[df['isCommercial'] == True]
    residential = df[df['isCommercial'] == False]

    print(f"üìä –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤: {len(commercial)}")
    print(f"üìä –ñ–∏–ª—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤: {len(residential)}")
    print(f"üìä –î–æ–ª—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö: {len(commercial)/len(df)*100:.1f}%")

    print("\nüî• –ö–õ–Æ–ß–ï–í–´–ï –†–ê–ó–õ–ò–ß–ò–Ø:")
    print("-" * 40)

    features_to_analyze = [
        'avg_consumption', 'min_consumption', 'max_consumption',
        'heating_season', 'summer_avg', 'winter_avg',
        'cv', 'summer_winter_ratio', 'heating_summer_ratio',
        'consumption_per_resident', 'consumption_per_area',
        'zero_months', 'quarter_stability', 'consumption_entropy',
        'consecutive_high', 'peak_months'
    ]

    results = {}

    for feature in features_to_analyze:
        if feature in df.columns:
            comm_mean = commercial[feature].mean()
            res_mean = residential[feature].mean()
            comm_median = commercial[feature].median()
            res_median = residential[feature].median()

            print(f"\nüìà {feature}:")
            print(
                f"   –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ: —Å—Ä–µ–¥–Ω–µ–µ={comm_mean:.1f}, –º–µ–¥–∏–∞–Ω–∞={comm_median:.1f}")
            print(
                f"   –ñ–∏–ª—ã–µ:        —Å—Ä–µ–¥–Ω–µ–µ={res_mean:.1f}, –º–µ–¥–∏–∞–Ω–∞={res_median:.1f}")
            print(f"   –†–∞–∑–Ω–∏—Ü–∞:      {(comm_mean/res_mean - 1)*100:+.1f}%")

            results[feature] = {
                'commercial_mean': comm_mean,
                'residential_mean': res_mean,
                'commercial_median': comm_median,
                'residential_median': res_median,
                'ratio': comm_mean / res_mean if res_mean > 0 else float('inf')
            }

    return results


def find_optimal_thresholds(df):
    """–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
    print("\nüéØ –ü–û–ò–°–ö –û–ü–¢–ò–ú–ê–õ–¨–ù–´–• –ü–û–†–û–ì–û–í–´–• –ó–ù–ê–ß–ï–ù–ò–ô")
    print("=" * 60)

    commercial = df[df['isCommercial'] == True]
    residential = df[df['isCommercial'] == False]

    rules = {}

    # 1. –ü—Ä–∞–≤–∏–ª–æ –ø–æ –æ—Ç–æ–ø–∏—Ç–µ–ª—å–Ω–æ–º—É —Å–µ–∑–æ–Ω—É (–∫–ª—é—á–µ–≤–æ–µ!)
    heating_values = df['heating_season'].values
    comm_heating = commercial['heating_season'].values
    res_heating = residential['heating_season'].values

    # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
    thresholds = [1000, 1500, 2000, 2500, 3000, 3500, 4000, 5000]
    best_threshold = None
    best_score = 0

    print("\nüå°Ô∏è –ê–Ω–∞–ª–∏–∑ –ø–æ—Ä–æ–≥–∞ –¥–ª—è –æ—Ç–æ–ø–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–µ–∑–æ–Ω–∞:")
    for threshold in thresholds:
        tp = np.sum((comm_heating >= threshold))  # True Positive
        fp = np.sum((res_heating >= threshold))   # False Positive
        fn = np.sum((comm_heating < threshold))   # False Negative

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / \
            (precision + recall) if (precision + recall) > 0 else 0

        print(
            f"   {threshold:4d} –∫–í—Ç¬∑—á: —Ç–æ—á–Ω–æ—Å—Ç—å={precision:.3f}, –ø–æ–∫—Ä—ã—Ç–∏–µ={recall:.3f}, F1={f1:.3f}")

        if f1 > best_score:
            best_score = f1
            best_threshold = threshold

    rules['heating_season_threshold'] = {
        'value': best_threshold,
        'description': f'–°—Ä–µ–¥–Ω–µ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –≤ –æ—Ç–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–∑–æ–Ω > {best_threshold} –∫–í—Ç¬∑—á',
        'precision': precision,
        'recall': recall,
        'f1': best_score
    }

    # 2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ—Ä–æ–≥–æ–≤ –¥–ª—è –¥—Ä—É–≥–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features_to_optimize = [
        'min_consumption', 'consumption_per_resident', 'cv',
        'consumption_entropy', 'consecutive_high', 'quarter_stability'
    ]

    for feature in features_to_optimize:
        if feature in df.columns:
            best_thresh, best_f1, best_prec, best_rec = optimize_threshold(
                df, feature)
            rules[f'{feature}_threshold'] = {
                'value': best_thresh,
                'description': f'{feature} –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥',
                'precision': best_prec,
                'recall': best_rec,
                'f1': best_f1
            }

    return rules


def optimize_threshold(df, feature):
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–∞"""
    commercial = df[df['isCommercial'] == True]
    residential = df[df['isCommercial'] == False]

    comm_values = commercial[feature].dropna()
    res_values = residential[feature].dropna()

    if len(comm_values) == 0 or len(res_values) == 0:
        return 0, 0, 0, 0

    # –ü—Ä–æ–±—É–µ–º –ø–æ—Ä–æ–≥–∏ –æ—Ç 10% –¥–æ 90% –∫–≤–∞–Ω—Ç–∏–ª–µ–π
    percentiles = np.arange(10, 91, 5)
    best_f1 = 0
    best_threshold = 0
    best_precision = 0
    best_recall = 0

    for p in percentiles:
        threshold = np.percentile(df[feature].dropna(), p)

        # –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞—Ç–Ω–∞—è (–º–µ–Ω—å—à–µ = –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ)
        if feature in ['cv', 'consumption_entropy', 'quarter_stability']:
            mask = df[feature] <= threshold
        else:
            mask = df[feature] >= threshold

        precision, recall = calculate_rule_metrics(df, mask)
        f1 = 2 * precision * recall / \
            (precision + recall) if (precision + recall) > 0 else 0

        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
            best_precision = precision
            best_recall = recall

    return best_threshold, best_f1, best_precision, best_recall


def test_rules_effectiveness(df, rules):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª"""
    print("\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò –ü–†–ê–í–ò–õ")
    print("=" * 60)

    commercial = df[df['isCommercial'] == True]
    residential = df[df['isCommercial'] == False]

    # –ü—Ä–∞–≤–∏–ª–æ 1: –û—Ç–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–∑–æ–Ω
    rule1_mask = df['heating_season'] >= rules['heating_season_threshold']['value']
    tp1 = np.sum(rule1_mask & (df['isCommercial'] == True))
    fp1 = np.sum(rule1_mask & (df['isCommercial'] == False))
    precision1 = tp1 / (tp1 + fp1) if (tp1 + fp1) > 0 else 0
    recall1 = tp1 / len(commercial) if len(commercial) > 0 else 0

    print(
        f"üìè –ü—Ä–∞–≤–∏–ª–æ 1 - –û—Ç–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–∑–æ–Ω > {rules['heating_season_threshold']['value']} –∫–í—Ç¬∑—á:")
    print(f"   –ù–∞–π–¥–µ–Ω–æ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö: {tp1}/{len(commercial)} ({recall1:.1%})")
    print(
        f"   –õ–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π: {fp1}/{len(residential)} ({fp1/len(residential):.1%})")
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {precision1:.1%}")

    effectiveness = {
        'rule1': {'precision': precision1, 'recall': recall1, 'caught': tp1}
    }

    # –î–æ–±–∞–≤–ª—è–µ–º –¥—Ä—É–≥–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
    rule_counter = 2
    for rule_name, rule_info in rules.items():
        if rule_name != 'heating_season_threshold' and '_threshold' in rule_name:
            feature = rule_name.replace('_threshold', '')
            if feature in df.columns:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–æ
                if feature in ['cv', 'consumption_entropy', 'quarter_stability']:
                    mask = df[feature] <= rule_info['value']
                    comparison = '<='
                else:
                    mask = df[feature] >= rule_info['value']
                    comparison = '>='

                tp = np.sum(mask & (df['isCommercial'] == True))
                fp = np.sum(mask & (df['isCommercial'] == False))
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / len(commercial) if len(commercial) > 0 else 0

                print(
                    f"\nüìè –ü—Ä–∞–≤–∏–ª–æ {rule_counter} - {feature} {comparison} {rule_info['value']:.2f}:")
                print(
                    f"   –ù–∞–π–¥–µ–Ω–æ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö: {tp}/{len(commercial)} ({recall:.1%})")
                print(
                    f"   –õ–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π: {fp}/{len(residential)} ({fp/len(residential):.1%})")
                print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {precision:.1%}")

                effectiveness[f'rule{rule_counter}'] = {
                    'precision': precision, 'recall': recall, 'caught': tp,
                    'feature': feature, 'threshold': rule_info['value']
                }
                rule_counter += 1

    return effectiveness


def generate_config_code(rules, hidden_patterns):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –¥–ª—è config.py —Å —É—á–µ—Ç–æ–º —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    print("\nüíæ –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–ë–ù–û–í–õ–ï–ù–ù–û–ì–û –ö–û–î–ê –î–õ–Ø CONFIG.PY")
    print("=" * 60)

    config_code = f"""
# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
FRAUD_RULES = {{
    'rule1': {{
        'description': '–í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –≤ –æ—Ç–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–∑–æ–Ω > {rules['heating_season_threshold']['value']} –∫–í—Ç¬∑—á (–∫–ª—é—á–µ–≤–æ–π –∫—Ä–∏—Ç–µ—Ä–∏–π)',
        'condition': lambda df: df['heating_season'] > {rules['heating_season_threshold']['value']}
    }},"""

    rule_counter = 2
    for rule_name, rule_info in rules.items():
        if rule_name != 'heating_season_threshold' and '_threshold' in rule_name:
            feature = rule_name.replace('_threshold', '')
            if feature in ['cv', 'consumption_entropy', 'quarter_stability']:
                operator = '<'
                comparison = 'lambda df: df[\'{}\'] < {:.3f}'.format(
                    feature, rule_info['value'])
            else:
                operator = '>'
                comparison = 'lambda df: df[\'{}\'] > {:.3f}'.format(
                    feature, rule_info['value'])

            config_code += f"""
    'rule{rule_counter}': {{
        'description': '{feature} {operator} {rule_info["value"]:.3f} (F1={rule_info["f1"]:.3f})',
        'condition': {comparison}
    }},"""
            rule_counter += 1

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    if 'combination_rules' in hidden_patterns:
        # –¢–æ–ø-3
        for i, combo_rule in enumerate(hidden_patterns['combination_rules'][:3]):
            config_code += f"""
    'combo_rule{i+1}': {{
        'description': '{combo_rule["rule"]} (—Ç–æ—á–Ω–æ—Å—Ç—å={combo_rule["precision"]:.2%})',
        'condition': lambda df: {combo_rule["condition"].replace('AND', '&').replace('OR', '|')}
    }},"""

    config_code += """
}

# –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ (–±—É–¥—É—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
MODEL_METRICS = {
    'last_updated': None,
    'accuracy': 0.0,
    'precision': 0.0,
    'recall': 0.0,
    'f1_score': 0.0,
    'auc_roc': 0.0,
    'total_features': 0,
    'most_important_features': [],
    'confusion_matrix': None
}

# –°–∫—Ä—ã—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –≤ –¥–∞–Ω–Ω—ã—Ö
HIDDEN_PATTERNS = """ + str(hidden_patterns) + """
"""

    print(config_code)
    return config_code


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    print("üîç –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –î–õ–Ø –§–û–†–ú–ò–†–û–í–ê–ù–ò–Ø –ü–†–ê–í–ò–õ –û–ë–ù–ê–†–£–ñ–ï–ù–ò–Ø")
    print("=" * 80)

    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    train_path = "data/dataset_train.json"

    if not Path(train_path).exists():
        print(f"‚ùå –§–∞–π–ª {train_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("üì• –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–∫–∞—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —Å—Å—ã–ª–∫–∞–º –∏–∑ –∫–µ–π—Å–∞ –∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ –≤ –ø–∞–ø–∫—É data/")
        return

    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = load_and_analyze_dataset(train_path)

        # 2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (–≤–∫–ª—é—á–∞—è –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
        df = calculate_consumption_features(df)

        # 3. –ü–æ–∏—Å–∫ —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ !!!
        hidden_patterns = discover_hidden_patterns(df)

        # 4. –ê–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        patterns = analyze_commercial_patterns(df)

        # 5. –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤
        rules = find_optimal_thresholds(df)

        # 6. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª
        effectiveness = test_rules_effectiveness(df, rules)

        # 7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞
        config_code = generate_config_code(rules, hidden_patterns)

        # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = {
            'rules': rules,
            'effectiveness': effectiveness,
            'patterns': patterns,
            'hidden_patterns': hidden_patterns,
            'config_code': config_code,
            'dataset_stats': {
                'total_objects': len(df),
                'commercial_objects': df['isCommercial'].sum(),
                'fraud_rate': df['isCommercial'].mean(),
                'features_analyzed': len(df.select_dtypes(include=[np.number]).columns)
            }
        }

        with open('rules_analysis_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)

        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ rules_analysis_results.json")
        print(f"üìä –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(df)} –æ–±—ä–µ–∫—Ç–æ–≤")
        print(f"üéØ –ù–∞–π–¥–µ–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª: {len(rules)}")
        print(f"üïµÔ∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(hidden_patterns)}")
        print(
            f"üßÆ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(df.select_dtypes(include=[np.number]).columns)}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
