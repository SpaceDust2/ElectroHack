"""
ü§ñ –ú–æ–¥—É–ª—å –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
"""

from datetime import datetime
from feature_engineering import FeatureEngineer
from config import MODEL_PARAMS
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, cross_val_score, learning_curve
from sklearn.metrics import (roc_auc_score, accuracy_score, classification_report,
                             confusion_matrix, precision_recall_curve, roc_curve,
                             precision_score, recall_score, f1_score)
from sklearn.ensemble import RandomForestClassifier
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
import joblib
from typing import Dict, List, Tuple, Any
import warnings
import os
warnings.filterwarnings('ignore')


class ModelTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""

    def __init__(self, use_gpu: bool = True):
        self.use_gpu = use_gpu
        self.models = self._init_models()
        self.feature_engineer = FeatureEngineer()
        self.best_model = None
        self.best_score = 0
        self.results = {}
        self.detailed_metrics = {}

    def _init_models(self) -> Dict[str, Any]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
        models = {}

        # CatBoost
        catboost_params = MODEL_PARAMS['catboost'].copy()
        if not self.use_gpu:
            catboost_params['task_type'] = 'CPU'
        models['CatBoost'] = CatBoostClassifier(**catboost_params)

        # XGBoost
        xgboost_params = MODEL_PARAMS['xgboost'].copy()
        if not self.use_gpu:
            xgboost_params['tree_method'] = 'hist'
        models['XGBoost'] = XGBClassifier(**xgboost_params)

        # Random Forest
        models['RandomForest'] = RandomForestClassifier(
            **MODEL_PARAMS['random_forest'])

        return models

    def load_data(self, train_path: str, test_path: str = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        with open(train_path, 'r', encoding='utf-8') as f:
            train_data = json.load(f)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("üîß Feature engineering...")
        train_df = self.feature_engineer.extract_features(train_data)

        test_df = None
        if test_path:
            with open(test_path, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            test_df = self.feature_engineer.extract_features(test_data)

        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(train_df)} train")
        if test_df is not None:
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(test_df)} test")
        print(f"‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_engineer.feature_names)}")

        # –ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        fraud_rate = train_df['isCommercial'].mean()
        print(f"üìä –î–æ–ª—è –º–æ—à–µ–Ω–Ω–∏–∫–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ: {fraud_rate:.2%}")
        print(
            f"üìä –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ - –ß–µ—Å—Ç–Ω—ã–µ: {(1-fraud_rate)*100:.1f}%, –ú–æ—à–µ–Ω–Ω–∏–∫–∏: {fraud_rate*100:.1f}%")

        return train_df, test_df

    def train_models(self, train_df: pd.DataFrame, cv_folds: int = 5) -> Dict[str, Dict]:
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        print("\nüöÄ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –° –î–ï–¢–ê–õ–¨–ù–´–ú –ê–ù–ê–õ–ò–ó–û–ú")
        print("=" * 60)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
        if len(train_df) == 0:
            raise ValueError("‚ùå –û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã!")

        if 'isCommercial' not in train_df.columns:
            raise ValueError("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü 'isCommercial' –≤ –¥–∞–Ω–Ω—ã—Ö!")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X = train_df[self.feature_engineer.feature_names].fillna(0)
        y = train_df['isCommercial'].astype(int)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        if len(y.unique()) < 2:
            raise ValueError(
                "‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–Ω—É–∂–Ω—ã –∏ –º–æ—à–µ–Ω–Ω–∏–∫–∏, –∏ —á–µ—Å—Ç–Ω—ã–µ)!")

        # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

        for model_name, model in self.models.items():
            print(f"\nü§ñ –û–±—É—á–µ–Ω–∏–µ {model_name}")
            print("-" * 40)

            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
            cv_scores = []
            cv_predictions = []
            cv_probabilities = []
            cv_true_labels = []

            for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
                X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

                # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ñ–æ–ª–¥–µ
                model_copy = self._clone_model(model)
                model_copy.fit(X_train_fold, y_train_fold)

                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                y_pred_proba = model_copy.predict_proba(X_val_fold)[:, 1]
                y_pred = model_copy.predict(X_val_fold)

                # –ú–µ—Ç—Ä–∏–∫–∏
                auc = roc_auc_score(y_val_fold, y_pred_proba)
                cv_scores.append(auc)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ–±—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                cv_predictions.extend(y_pred)
                cv_probabilities.extend(y_pred_proba)
                cv_true_labels.extend(y_val_fold)

                print(f"   Fold {fold+1}: AUC = {auc:.4f}")

            # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            model.fit(X, y)

            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
            cv_predictions = np.array(cv_predictions)
            cv_probabilities = np.array(cv_probabilities)
            cv_true_labels = np.array(cv_true_labels)

            detailed_cv_metrics = self._calculate_detailed_metrics(
                cv_true_labels, cv_predictions, cv_probabilities)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            cv_scores_array = np.array(cv_scores)
            self.results[model_name] = {
                'model': model,
                'cv_scores': cv_scores_array,
                'cv_mean': cv_scores_array.mean(),
                'cv_std': cv_scores_array.std(),
                'feature_names': self.feature_engineer.feature_names,
                'detailed_metrics': detailed_cv_metrics
            }

            # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_importance = self._get_model_feature_importance(
                model, model_name)
            self.results[model_name]['feature_importance'] = feature_importance

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ {model_name}:")
            print(
                f"   AUC (CV): {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {detailed_cv_metrics['accuracy']:.4f}")
            print(f"   Precision: {detailed_cv_metrics['precision']:.4f}")
            print(f"   Recall: {detailed_cv_metrics['recall']:.4f}")
            print(f"   F1-Score: {detailed_cv_metrics['f1']:.4f}")

            # Confusion Matrix
            print(f"\n   Confusion Matrix:")
            cm = detailed_cv_metrics['confusion_matrix']
            print(f"   TN={cm[0, 0]:4d}  FP={cm[0, 1]:4d}")
            print(f"   FN={cm[1, 0]:4d}  TP={cm[1, 1]:4d}")

            # –¢–æ–ø-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if len(feature_importance) > 0:
                print(f"\n   –¢–æ–ø-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
                for i, (feature, importance) in enumerate(feature_importance.head(5).values):
                    print(f"   {i+1}. {feature}: {importance:.4f}")

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if np.mean(cv_scores) > self.best_score:
                self.best_score = np.mean(cv_scores)
                self.best_model = model_name

        print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {self.best_model}")
        print(f"üéØ –õ–£–ß–®–ò–ô AUC: {self.best_score:.4f}")

        return self.results

    def _clone_model(self, model):
        """–ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if isinstance(model, CatBoostClassifier):
            return CatBoostClassifier(**model.get_params())
        elif isinstance(model, XGBClassifier):
            return XGBClassifier(**model.get_params())
        else:
            return RandomForestClassifier(**model.get_params())

    def _calculate_detailed_metrics(self, y_true, y_pred, y_prob):
        """–†–∞—Å—á–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1': f1_score(y_true, y_pred, zero_division=0),
            'auc': roc_auc_score(y_true, y_prob),
            'confusion_matrix': confusion_matrix(y_true, y_pred)
        }

    def _get_model_feature_importance(self, model, model_name):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        try:
            feature_names = self.feature_engineer.feature_names

            if not feature_names:
                print(f"‚ö†Ô∏è –ù–µ—Ç —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {model_name}")
                return pd.DataFrame()

            importances = None

            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
            elif hasattr(model, 'get_feature_importance'):
                importances = model.get_feature_importance()
            else:
                print(f"‚ö†Ô∏è {model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                return pd.DataFrame()

            if importances is None or len(importances) == 0:
                print(f"‚ö†Ô∏è –ü—É—Å—Ç–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {model_name}")
                return pd.DataFrame()

            if len(importances) != len(feature_names):
                print(
                    f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤: {len(importances)} –≤–∞–∂–Ω–æ—Å—Ç–µ–π vs {len(feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                return pd.DataFrame()

            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False)

            return importance_df

        except Exception as e:
            print(
                f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {model_name}: {e}")
            return pd.DataFrame()

    def evaluate_on_test(self, test_df: pd.DataFrame) -> Dict[str, Dict]:
        """–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        if not self.results:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª–∏!")

        print("\nüìè –î–ï–¢–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–û–ú –î–ê–¢–ê–°–ï–¢–ï")
        print("=" * 60)

        X_test = test_df[self.feature_engineer.feature_names].fillna(0)
        y_test = test_df['isCommercial'].astype(int)

        test_results = {}

        for model_name, model_info in self.results.items():
            print(f"\nüî¨ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {model_name}")
            print("-" * 40)

            model = model_info['model']

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            y_pred = model.predict(X_test)

            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            detailed_metrics = self._calculate_detailed_metrics(
                y_test, y_pred, y_pred_proba)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            classification_rep = classification_report(
                y_test, y_pred, output_dict=True, zero_division=0)

            test_results[model_name] = {
                **detailed_metrics,
                'predictions': y_pred,
                'probabilities': y_pred_proba,
                'classification_report': classification_rep
            }

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print(f"   AUC: {detailed_metrics['auc']:.4f}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {detailed_metrics['accuracy']:.4f}")
            print(f"   Precision: {detailed_metrics['precision']:.4f}")
            print(f"   Recall: {detailed_metrics['recall']:.4f}")
            print(f"   F1-Score: {detailed_metrics['f1']:.4f}")

            # Confusion Matrix
            print(f"   Confusion Matrix:")
            cm = detailed_metrics['confusion_matrix']
            print(f"   TN={cm[0, 0]:4d}  FP={cm[0, 1]:4d}")
            print(f"   FN={cm[1, 0]:4d}  TP={cm[1, 1]:4d}")

            # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
            if len(y_test) > 0:
                false_positives = np.sum((y_test == 0) & (y_pred == 1))
                false_negatives = np.sum((y_test == 1) & (y_pred == 0))
                print(
                    f"   –õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è: {false_positives} ({false_positives/len(y_test)*100:.1f}%)")
                print(
                    f"   –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –º–æ—à–µ–Ω–Ω–∏–∫–∏: {false_negatives} ({false_negatives/len(y_test)*100:.1f}%)")

        return test_results

    def generate_comprehensive_report(self, test_results=None) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        print("\nüìã –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –û–¢–ß–ï–¢–ê")
        print("=" * 60)

        try:
            if not self.results:
                raise ValueError(
                    "‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞!")

            report = {
                'training_completed_at': datetime.now().isoformat(),
                'best_model': self.best_model if self.best_model else 'Unknown',
                'best_score': float(self.best_score) if self.best_score else 0.0,
                'total_features': len(self.feature_engineer.feature_names) if self.feature_engineer.feature_names else 0,
                'gpu_used': self.use_gpu,
                'models_results': {}
            }

            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
            for model_name, model_info in self.results.items():
                try:
                    model_result = {
                        'cv_mean_auc': float(model_info.get('cv_mean', 0)),
                        'cv_std_auc': float(model_info.get('cv_std', 0)),
                        'cv_scores': model_info.get('cv_scores', []).tolist() if hasattr(model_info.get('cv_scores', []), 'tolist') else [],
                        'detailed_cv_metrics': {}
                    }

                    # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                    if 'detailed_metrics' in model_info and model_info['detailed_metrics']:
                        for k, v in model_info['detailed_metrics'].items():
                            try:
                                if np.isscalar(v):
                                    model_result['detailed_cv_metrics'][k] = float(
                                        v)
                                elif hasattr(v, 'tolist'):
                                    model_result['detailed_cv_metrics'][k] = v.tolist(
                                    )
                                else:
                                    model_result['detailed_cv_metrics'][k] = str(
                                        v)
                            except Exception as e:
                                print(
                                    f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–µ—Ç—Ä–∏–∫–∏ {k}: {e}")
                                model_result['detailed_cv_metrics'][k] = None

                    # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    if 'feature_importance' in model_info and len(model_info['feature_importance']) > 0:
                        try:
                            model_result['top_features'] = model_info['feature_importance'].head(
                                15).to_dict('records')
                        except Exception as e:
                            print(
                                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {model_name}: {e}")
                            model_result['top_features'] = []

                    report['models_results'][model_name] = model_result

                except Exception as e:
                    print(
                        f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                    continue

            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            if test_results:
                report['test_results'] = {}
                for model_name, test_result in test_results.items():
                    try:
                        report['test_results'][model_name] = {
                            'auc': float(test_result.get('auc', 0)),
                            'accuracy': float(test_result.get('accuracy', 0)),
                            'precision': float(test_result.get('precision', 0)),
                            'recall': float(test_result.get('recall', 0)),
                            'f1': float(test_result.get('f1', 0)),
                            'confusion_matrix': test_result.get('confusion_matrix', [[0, 0], [0, 0]]).tolist() if hasattr(test_result.get('confusion_matrix', [[0, 0], [0, 0]]), 'tolist') else [[0, 0], [0, 0]],
                            'classification_report': test_result.get('classification_report', {})
                        }
                    except Exception as e:
                        print(
                            f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è {model_name}: {e}")
                        continue

            # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å - –¥–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            if self.best_model and self.best_model in self.results:
                try:
                    best_model_info = self.results[self.best_model]
                    report['best_model_details'] = {}

                    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    if 'feature_importance' in best_model_info and len(best_model_info['feature_importance']) > 0:
                        try:
                            report['best_model_details']['feature_importance'] = best_model_info['feature_importance'].head(
                                20).to_dict('records')
                        except Exception as e:
                            print(
                                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏: {e}")
                            report['best_model_details']['feature_importance'] = []

                    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ CV
                    if 'detailed_metrics' in best_model_info:
                        report['best_model_details']['cv_detailed_metrics'] = {}
                        for k, v in best_model_info['detailed_metrics'].items():
                            try:
                                if np.isscalar(v):
                                    report['best_model_details']['cv_detailed_metrics'][k] = float(
                                        v)
                                elif hasattr(v, 'tolist'):
                                    report['best_model_details']['cv_detailed_metrics'][k] = v.tolist(
                                    )
                                else:
                                    report['best_model_details']['cv_detailed_metrics'][k] = str(
                                        v)
                            except Exception as e:
                                print(
                                    f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ {k}: {e}")

                except Exception as e:
                    print(
                        f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–µ—Ç–∞–ª–µ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏: {e}")
                    report['best_model_details'] = {}

            # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
            try:
                class_distribution = self._analyze_class_distribution()
                total_samples = 0

                if self.best_model and self.best_model in self.results:
                    cm = self.results[self.best_model].get(
                        'detailed_metrics', {}).get('confusion_matrix')
                    if cm is not None:
                        total_samples = int(cm.sum())

                report['data_analysis'] = {
                    'total_samples': total_samples,
                    'class_distribution': class_distribution
                }
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
                report['data_analysis'] = {
                    'total_samples': 0,
                    'class_distribution': {}
                }

            print(f"‚úÖ –û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è {len(self.results)} –º–æ–¥–µ–ª–µ–π")
            return report

        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            return {
                'training_completed_at': datetime.now().isoformat(),
                'best_model': 'Unknown',
                'best_score': 0.0,
                'total_features': 0,
                'gpu_used': self.use_gpu,
                'models_results': {},
                'error': str(e)
            }

    def _analyze_class_distribution(self):
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤"""
        try:
            if not self.best_model or self.best_model not in self.results:
                print("‚ö†Ô∏è –ù–µ—Ç –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤")
                return {}

            model_info = self.results[self.best_model]

            if 'detailed_metrics' not in model_info:
                print("‚ö†Ô∏è –ù–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Å–æ–≤")
                return {}

            if 'confusion_matrix' not in model_info['detailed_metrics']:
                print("‚ö†Ô∏è –ù–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Å–æ–≤")
                return {}

            cm = model_info['detailed_metrics']['confusion_matrix']

            if cm is None:
                print("‚ö†Ô∏è –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ø—É—Å—Ç–∞")
                return {}

            total = cm.sum()

            if total == 0:
                print("‚ö†Ô∏è –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –Ω—É–ª–∏")
                return {}

            return {
                'total_samples': int(total),
                'negative_samples': int(cm[0].sum()),
                'positive_samples': int(cm[1].sum()),
                'positive_rate': float(cm[1].sum() / total) if total > 0 else 0.0
            }

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤: {e}")
            return {}

    def update_config_with_metrics(self, report):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ config.py —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        print("\n‚öôÔ∏è –û–ë–ù–û–í–õ–ï–ù–ò–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò")
        print("=" * 40)

        try:
            if not self.best_model:
                print("‚ö†Ô∏è –ù–µ—Ç –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
                return

            if 'models_results' not in report or self.best_model not in report['models_results']:
                print(f"‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ {self.best_model}")
                return

            best_metrics = report['models_results'][self.best_model].get(
                'detailed_cv_metrics', {})

            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            accuracy = best_metrics.get('accuracy', 0.0)
            precision = best_metrics.get('precision', 0.0)
            recall = best_metrics.get('recall', 0.0)
            f1 = best_metrics.get('f1', 0.0)
            confusion_matrix = best_metrics.get(
                'confusion_matrix', [[0, 0], [0, 0]])

            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            total_features = report.get('total_features', 0)
            best_score = report.get('best_score', 0.0)

            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_importance = []
            if 'best_model_details' in report and 'feature_importance' in report['best_model_details']:
                feature_importance = report['best_model_details']['feature_importance'][:10]

            # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
            total_samples = 0
            positive_rate = 0.0

            if 'data_analysis' in report:
                total_samples = report['data_analysis'].get('total_samples', 0)
                if 'class_distribution' in report['data_analysis']:
                    positive_rate = report['data_analysis']['class_distribution'].get(
                        'positive_rate', 0.0)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è MODEL_METRICS –≤ config.py
            metrics_update = f"""
# –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ (–æ–±–Ω–æ–≤–ª–µ–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})
MODEL_METRICS = {{
    'last_updated': '{datetime.now().isoformat()}',
    'model_name': '{self.best_model}',
    'accuracy': {accuracy:.4f},
    'precision': {precision:.4f},
    'recall': {recall:.4f},
    'f1_score': {f1:.4f},
    'auc_roc': {best_score:.4f},
    'total_features': {total_features},
    'most_important_features': {feature_importance},
    'confusion_matrix': {confusion_matrix},
    'samples_analyzed': {total_samples},
    'fraud_rate_detected': {positive_rate:.3f}
}}
"""

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è config.py
            with open('model_metrics_update.py', 'w', encoding='utf-8') as f:
                f.write(metrics_update)

            print("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ model_metrics_update.py")
            print("üìù –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ config.py –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è UI")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            print("‚ö†Ô∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞, –Ω–æ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")

    def save_model(self, model_name: str = None, filename: str = 'model.joblib') -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        try:
            if model_name is None:
                model_name = self.best_model

            if not model_name:
                raise ValueError("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω–æ –∏–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!")

            if model_name not in self.results:
                raise ValueError(
                    f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –æ–±—É—á–µ–Ω–∏—è!")

            model_info = self.results[model_name]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            if 'model' not in model_info:
                raise ValueError(
                    f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è {model_name}!")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–±—ä–µ–∫—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫
            model_data = {
                'model': model_info['model'],
                'feature_names': model_info.get('feature_names', []),
                'model_name': model_name,
                'cv_score': float(model_info.get('cv_mean', 0.0)),
                'feature_engineer': self.feature_engineer,
                'training_date': datetime.now().isoformat(),
                'detailed_metrics': model_info.get('detailed_metrics', {})
            }

            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            try:
                if 'feature_importance' in model_info and len(model_info['feature_importance']) > 0:
                    model_data['feature_importance'] = model_info['feature_importance'].to_dict(
                        'records')
                else:
                    model_data['feature_importance'] = []
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
                model_data['feature_importance'] = []

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            joblib.dump(model_data, filename)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filename}")
            print(
                f"üìä –í–∫–ª—é—á–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –∏ –≤–∞–∂–Ω–æ—Å—Ç—å {len(model_data['feature_names'])} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

            return filename

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise e

    def save_ensemble(self, filename: str = 'fraud_ensemble.joblib') -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞"""
        try:
            if not self.results:
                raise ValueError(
                    "‚ùå –ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è!")

            print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π...")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∞–Ω—Å–∞–º–±–ª—è
            ensemble_data = {
                'models': {},
                'best_model': self.best_model,
                'best_score': float(self.best_score),
                'feature_engineer': self.feature_engineer,
                'training_date': datetime.now().isoformat(),
                'ensemble_type': 'consensus_based',
                'total_models': len(self.results)
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å –∏—Ö –º–µ—Ç—Ä–∏–∫–∞–º–∏
            for model_name, model_info in self.results.items():
                try:
                    model_data = {
                        'model': model_info['model'],
                        'cv_score': float(model_info.get('cv_mean', 0.0)),
                        'cv_std': float(model_info.get('cv_std', 0.0)),
                        'detailed_metrics': model_info.get('detailed_metrics', {}),
                        'feature_names': model_info.get('feature_names', [])
                    }

                    # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    if 'feature_importance' in model_info and len(model_info['feature_importance']) > 0:
                        try:
                            model_data['feature_importance'] = model_info['feature_importance'].to_dict(
                                'records')
                        except Exception as e:
                            print(
                                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è {model_name}: {e}")
                            model_data['feature_importance'] = []
                    else:
                        model_data['feature_importance'] = []

                    ensemble_data['models'][model_name] = model_data
                    print(
                        f"  ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å {model_name} (AUC: {model_data['cv_score']:.4f})")

                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                    continue

            # –û–±—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            if self.feature_engineer and self.feature_engineer.feature_names:
                ensemble_data['feature_names'] = self.feature_engineer.feature_names
            else:
                ensemble_data['feature_names'] = []

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
            joblib.dump(ensemble_data, filename)

            print(
                f"üéâ –ê–Ω—Å–∞–º–±–ª—å –∏–∑ {len(ensemble_data['models'])} –º–æ–¥–µ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")
            print(f"üìä –ú–æ–¥–µ–ª–∏: {', '.join(ensemble_data['models'].keys())}")
            print(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {ensemble_data['best_model']}")

            return filename

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∞–Ω—Å–∞–º–±–ª—è: {e}")
            raise e


def train_pipeline(train_path: str, test_path: str = None,
                   model_filename: str = 'fraud_model.joblib',
                   use_gpu: bool = True) -> Dict:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""

    print("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê –û–ë–£–ß–ï–ù–ò–Ø ML")
    print("=" * 80)

    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        if not os.path.exists(train_path):
            raise FileNotFoundError(
                f"‚ùå –§–∞–π–ª –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {train_path}")

        if test_path and not os.path.exists(test_path):
            print(
                f"‚ö†Ô∏è –§–∞–π–ª —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {test_path}, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            test_path = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
        trainer = ModelTrainer(use_gpu=use_gpu)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        train_df, test_df = trainer.load_data(train_path, test_path)

        if len(train_df) == 0:
            raise ValueError("‚ùå –û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏!")

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        print("ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        trainer.train_models(train_df)

        if not trainer.results:
            raise ValueError("‚ùå –ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
        test_results = None
        if test_df is not None and len(test_df) > 0:
            print("üß™ –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            try:
                test_results = trainer.evaluate_on_test(test_df)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
                print("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        try:
            trainer.save_model(filename=model_filename)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞, –Ω–æ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –¥–ª—è –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞
        print("ü§ù –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π...")
        try:
            ensemble_filename = model_filename.replace(
                '.joblib', '_ensemble.joblib')
            trainer.save_ensemble(filename=ensemble_filename)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∞–Ω—Å–∞–º–±–ª—è: {e}")
            print("‚ö†Ô∏è –ê–Ω—Å–∞–º–±–ª—å –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω, –Ω–æ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        print("üìã –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞...")
        report = trainer.generate_comprehensive_report(test_results)

        if not report or 'error' in report:
            print("‚ö†Ô∏è –û—à–∏–±–∫–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞, —Å–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç")
            report = {
                'training_completed_at': datetime.now().isoformat(),
                'best_model': trainer.best_model if trainer.best_model else 'Unknown',
                'best_score': float(trainer.best_score) if trainer.best_score else 0.0,
                'total_features': len(trainer.feature_engineer.feature_names) if trainer.feature_engineer.feature_names else 0,
                'gpu_used': use_gpu,
                'models_results': {},
                'status': 'completed_with_warnings'
            }

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        print("‚öôÔ∏è –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
        try:
            trainer.update_config_with_metrics(report)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        print("üìÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞...")
        try:
            with open('training_report.json', 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")

        print(f"\nüéâ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
        print(
            f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {report.get('best_model', 'Unknown')} (AUC: {report.get('best_score', 0):.4f})")
        print(f"üìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ training_report.json")
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_filename}")
        print(f"‚öôÔ∏è –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è UI —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ model_metrics_update.py")

        return report

    except Exception as e:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –í –ü–ê–ô–ü–õ–ê–ô–ù–ï: {e}")
        print("üìã –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–≤–∞—Ä–∏–π–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")

        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ–± –æ—à–∏–±–∫–µ
        error_report = {
            'training_completed_at': datetime.now().isoformat(),
            'status': 'failed',
            'error': str(e),
            'best_model': None,
            'best_score': 0.0,
            'total_features': 0,
            'gpu_used': use_gpu,
            'models_results': {}
        }

        try:
            with open('training_error_report.json', 'w', encoding='utf-8') as f:
                json.dump(error_report, f, ensure_ascii=False, indent=2)
            print("üìÑ –û—Ç—á–µ—Ç –æ–± –æ—à–∏–±–∫–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ training_error_report.json")
        except:
            pass

        raise e


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    report = train_pipeline(
        train_path='data/dataset_train.json',
        test_path='data/dataset_test.json',
        model_filename='fraud_detection_model.joblib',
        use_gpu=True
    )
